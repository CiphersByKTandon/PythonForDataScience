{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j0FiyuFscYHl"
   },
   "source": [
    "# A dataset of air quality\n",
    "\n",
    "We are going to use a dataset of air pollution measurements in Beijing archived by the UCI. To read about the dataset visit the [UCI page](https://archive.ics.uci.edu/ml/datasets/Beijing+Multi-Site+Air-Quality+Data).\n",
    "\n",
    "We are going to:\n",
    "  1. Preprocess the data\n",
    "  2. Build our own Lasso-regression\n",
    "  3. Use sklearn's tools to perform regression on the data\n",
    "  \n",
    "Let's get started"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "id": "NWxSewmucYHp"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import linear_model\n",
    "from sklearn import linear_model\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import urllib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WWSGSGFfcYHs"
   },
   "source": [
    "We will be trying to predict the pollution (PM2.5) using:\n",
    "- temperature ('TEMP')\n",
    "- pressure ('PRES')\n",
    "- dew-point temperature ('DEWP')\n",
    "- precipitation ('RAIN')\n",
    "- wind direction ('wd')\n",
    "- wind speed ('WSPM')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "id": "QujKMUPtcYHu"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./data.csv', <http.client.HTTPMessage at 0x16403a710>)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "urllib.request.urlretrieve('https://drive.google.com/uc?id=1m1g4Xn1wMAGV_EU0Nh1HTI1ogA3-tqJk&export=download', './data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "id": "2qOVpf72cYHv"
   },
   "outputs": [],
   "source": [
    "raw_df = pd.read_csv('data.csv',index_col='No')\n",
    "\n",
    "#put the columns in a useful order\n",
    "raw_df = raw_df[['PM2.5', 'year', 'month', 'day', 'hour', 'PM10', 'SO2', 'NO2', 'CO',\n",
    "       'O3', 'TEMP', 'PRES', 'DEWP', 'RAIN', 'wd', 'WSPM', 'station']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oQKg3AFPcYHx"
   },
   "source": [
    "Some of the records are missing. We need to handle that before we can easily use the data with most ML tools.\n",
    "\n",
    "### Removing missing data\n",
    "\n",
    "We are going to handle this by dropping those rows which have a NaN in one of these columns: ['PM2.5','hour','TEMP','PRES','DEWP','RAIN','wd','WSPM'].\n",
    "\n",
    "Save the result in `nonull_df`.\n",
    "\n",
    "We can use `df.dropna` to do this. Pandas documentation on this method is [here](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.dropna.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "id": "k0cYywfFcYHy"
   },
   "outputs": [],
   "source": [
    "nonull_df = raw_df.dropna(subset=['PM2.5','hour','TEMP','PRES','DEWP','RAIN','wd','WSPM'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UNruaXp6cYHz"
   },
   "source": [
    "To check you've done it correctly, you could use `clean_df.isnull().sum()` to confirm that there are no NaN rows in the columns we're interested in:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "id": "zPWKkfzHcYH0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PM2.5        0\n",
       "year         0\n",
       "month        0\n",
       "day          0\n",
       "hour         0\n",
       "PM10        17\n",
       "SO2        242\n",
       "NO2        331\n",
       "CO         866\n",
       "O3         685\n",
       "TEMP         0\n",
       "PRES         0\n",
       "DEWP         0\n",
       "RAIN         0\n",
       "wd           0\n",
       "WSPM         0\n",
       "station      0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nonull_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8ePRfNOEcYH1"
   },
   "source": [
    "### Removing unwanted columns \n",
    "\n",
    "Let's remove the columns we're not going to be using. We can use `nonull_df.drop(list_of_column_names, axis=1)` to do this. We will drop: ['year','month','day','PM10','SO2','NO2','CO','O3','station'].\n",
    "\n",
    "Again, feel free to check if it's worked with `clean_df.isnull().sum()`, for example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "id": "edYxDc5NcYH1"
   },
   "outputs": [],
   "source": [
    "clean_df = nonull_df.drop(labels= ['year','month','day','PM10','SO2','NO2','CO','O3','station'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "id": "0FGsOUWbcYH2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(34284, 8)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#there should be 34284 rows left in your dataframe, and 8 columns \n",
    "clean_df.shape #=(34284, 8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UoskxkwycYH3"
   },
   "source": [
    "### Splitting the dataset\n",
    "\n",
    "Before designing any machine learning model, we need to set aside the test data. We will use the remaining training data for fitting the model. **It is important to remember that the test data has to be set aside before preprocessing**.\n",
    "\n",
    "Any preprocessing that you do has to be done only on the training data and several key statistics need to be saved for the test stage. Separating the dataset into training and test before any preprocessing has happened help us to recreate the real world scenario where we will deploy our system and for which the data will come without any preprocessing.\n",
    "\n",
    "Later we will be performing a grid search to select parameter values. To do this we'll do cross-validation, but rather than split the data into training and validation here we'll split it later. So for now we'll just split into:\n",
    "\n",
    "- The training (and validation) set will have 85% of the total observations, \n",
    "- The test set, the remaining 15%.\n",
    "\n",
    "To avoid unwanted correlations connecting the training and test, we will split these by time. So:\n",
    "\n",
    "- Take the first 85% of the rows from clean_df and put them in train_df, take the remaining 15% of the rows and put them in test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "id": "NjbrkiVucYH5"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train_df, test_df = train_test_split(clean_df, train_size=0.85, random_state= 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hn-utn5pcYH6"
   },
   "source": [
    "To check the sizes are correct, we can use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "id": "lYolbW7RcYH6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.8499883327499709, 0.15001166725002918)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_df)/len(clean_df),len(test_df)/len(clean_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xMvSVU3-cYH7"
   },
   "source": [
    "# Detour: Lasso Regression\n",
    "\n",
    "Later we will use the sklearn toolkit, but in this section **will develop our own code** to do the Lasso regression.\n",
    "\n",
    "### Ordinary Least Squares Regression\n",
    "\n",
    "First, let's just perform normal linear regression.\n",
    "\n",
    "We'll use a toy design matrix & labels to use to check our code works. We'll also specify a weight vector too, for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "id": "jRBjUyWgcYH7"
   },
   "outputs": [],
   "source": [
    "X = np.array([[0.0,0],[1,3],[2.2,3]])\n",
    "y = np.array([0.0,1,2])\n",
    "w = np.array([1.0,2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ou21m-UNcYH7"
   },
   "source": [
    "###  Prediction Function\n",
    "\n",
    "The first task is to write a function to make predictions. Complete this function for linear regression, i.e. the predictions for all our points $f(X,\\mathbf{w}) = X \\mathbf{w}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "id": "r2jQlwn0cYH8"
   },
   "outputs": [],
   "source": [
    "def prediction(X,w):\n",
    "    pred = np.dot(X, w)\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "id": "gEKkPNjgcYH8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#use this code to check you've written the right function\n",
    "np.all(prediction(X,w)==np.array([0. , 7. , 8.2])) #Should return 'True'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wha2MzM7cYH9"
   },
   "source": [
    "### Objective Function \n",
    "\n",
    "Now we need to write a function that returns the 'error'.\n",
    "We'll just do normal Ordinary Least Squares with linear regression, so if you remember the cost function for that is:\n",
    "\n",
    "$$E = \\sum_{i=1}^N \\big(y_i-f(\\mathbf{x}_i,\\mathbf{w}) \\big)^2$$\n",
    "\n",
    "Where $E$ is the error, $N$ the number of points, $y_i$ is one of the labels, $\\mathbf{x}_i$ is the input for that label, $\\mathbf{w}$ is the weight vector. $f$ is the prediction function you've already written. Or feel free to substitute in $\\mathbf{x}_i^\\top \\mathbf{w}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "id": "LKWQ2Y41cYH9"
   },
   "outputs": [],
   "source": [
    "def objective(X,y,w):\n",
    "    \"\"\"\n",
    "    Computes the sum squared error (for us to perform OLS linear regression).\n",
    "    \"\"\"\n",
    "    #Put answer here\n",
    "    temp1 = prediction(X,w)\n",
    "    temp2 = (y-temp1)\n",
    "    Error = np.dot(temp2.T, temp2)\n",
    "    return Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "id": "OBo1zJeicYH-"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#You can use this code to check you've written the right function\n",
    "objective(X,y,w)==74.44"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9BhH_-zocYH-"
   },
   "source": [
    "### Objective Function Gradient\n",
    "\n",
    "Now you need to find the derivative of the objective wrt the parameter vector. You've already done this in lectures, so remember the gradient of the error function (for linear regression) is:\n",
    "\n",
    "$$\\frac{\\partial E}{\\partial \\mathbf{w}} = 2 X^\\top X \\mathbf{w} - 2 X^\\top y$$\n",
    "\n",
    "Add code to do this here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "id": "O2TD0-v9cYH-"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([39.28, 73.2 ])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def objective_derivative(X,y,w):\n",
    "    \"\"\"\n",
    "    Computes the derivative of the sum squared error, wrt the parameters.\n",
    "    \"\"\"\n",
    "    #Put answer here   \n",
    "    derivative = -2*X.T@y + 2*X.T@X@w\n",
    "    return derivative\n",
    "\n",
    "objective_derivative(X,y,w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tgvPQtC0cYH_"
   },
   "source": [
    "To check your gradients are correct, we can estimate the gradient numerically:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "id": "9MV54TG3cYH_"
   },
   "outputs": [],
   "source": [
    "def numerical_objective_derivative(X,y,w):\n",
    "    \"\"\"\n",
    "    Computes a numerical approximation to the derivative of the sum squared error, wrt the parameters.\n",
    "    \"\"\"\n",
    "    g = np.zeros_like(w)\n",
    "    for i,wi in enumerate(w):\n",
    "        d = np.zeros_like(w)\n",
    "        d[i]=1e-6\n",
    "        g[i] = (objective(X,y,w+d)-objective(X,y,w-d))/2e-6\n",
    "    return g"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "42SCGt-0cYIA"
   },
   "source": [
    "The two gradient vectors should be approximately equal:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "id": "zuRzN4-xcYIA"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([39.28, 73.2 ]), array([39.28, 73.2 ]))"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "objective_derivative(X,y,w),numerical_objective_derivative(X,y,w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-58158ZjcYIA"
   },
   "source": [
    "### Optimise $\\mathbf{w}$ to minimise the objective \n",
    "\n",
    "Now you need to use the gradient function you've written to maximise $\\mathbf{w}$ using gradient descent.\n",
    "Start with a sensible choice of w. You'll need to loop lots of times (e.g. 1000). At each iteration: compute the gradient and subtract the scaled gradient from the w parameter (you'll need to scale it by the learning rate, of e.g. 0.01)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "id": "eq9GtHEAcYIB"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.83333333, 0.05555556])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def optimise_parameters(X,y,startw):\n",
    "    \"\"\"\n",
    "    Returns the w that minimises the objective.\n",
    "    \"\"\"\n",
    "    #Put answer here\n",
    "    for a in range (2000):\n",
    "        grad = objective_derivative(X,y,startw)\n",
    "        startw = startw - grad*0.01\n",
    "    return startw\n",
    "\n",
    "optimise_parameters(X,y,w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "id": "tmbmC6B5cYIB"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.83333333 0.05555556]\n"
     ]
    }
   ],
   "source": [
    "bestw = optimise_parameters(X,y,w)\n",
    "print(bestw) #print our solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TG713sjPcYIB"
   },
   "source": [
    "Let's compare this to the answer provided by sklearn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "id": "0qzeBjibcYIC"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.83333333 0.05555556]\n"
     ]
    }
   ],
   "source": [
    "from sklearn import linear_model\n",
    "clf = linear_model.LinearRegression(fit_intercept=False)\n",
    "clf.fit(X,y)\n",
    "print(clf.coef_) #matches the value of w we found above, hopefully!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Pn_4IihFcYIC"
   },
   "source": [
    "## Lasso Regression\n",
    "\n",
    "###  New Objective Function\n",
    "\n",
    "We're now going to regularise the regression using $L_1$ regularisation - i.e. Lasso regression.\n",
    "\n",
    "We need a **new objective function**:\n",
    "\n",
    "$$E = \\frac{1}{2N}\\sum_{i=1}^N \\big(y_i-f(\\mathbf{x}_i,\\mathbf{w}) \\big)^2 + \\alpha \\sum_{j=1}^P |w_j|$$\n",
    "\n",
    "This is similar to before (but the first term is now half the mean squared error, rather than the sum squared error). The second term is the L1 regularisation term."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "id": "dl24ORnocYID"
   },
   "outputs": [],
   "source": [
    "def objective_lasso(X,y,w,alpha):\n",
    "    \"\"\"\n",
    "    Computes half the mean squared error, with an additional L1 regularising term. alpha controls the level of regularisation.\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    temp1=prediction(X,w)\n",
    "    temp2 = (y-temp1)\n",
    "    \n",
    "    Error = (np.dot(temp2.T, temp2))/(2*len(X)) + alpha * (sum(abs(w)))\n",
    "    \n",
    "    return Error\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FL_qwDiocYID"
   },
   "source": [
    "### The gradient of the lasso regression objective\n",
    "\n",
    "The tricky bit the derivative of the objective.\n",
    "\n",
    "The first part is similar to before. So, with the regularising term, the derivative is:\n",
    "$$\\frac{\\partial E}{\\partial \\mathbf{w}} = \\frac{1}{N}(X^\\top X \\mathbf{w} - X^\\top y) + \\alpha\\;\\text{sign}(\\mathbf{w})$$\n",
    "\n",
    "where $\\text{sign}(\\mathbf{w})$ returns a vector of the same shape as $\\mathbf{w}$ with +1 if the element is positive and -1 if it's negative. The `np.sign` method does this for you.\n",
    "\n",
    "Have a think about why this is (think about what differentiating the 'absolute' function $|w_j|$ involves - think about what happens when it's positive vs when it's negative. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "id": "0cCZM2kDcYID"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 6.64666667, 12.3       ])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def objective_lasso_derivative(X,y,w,alpha):\n",
    "    \"\"\"\n",
    "    Returns the derivative of the Lasso objective function.\n",
    "    \"\"\"\n",
    "    #Put answer here\n",
    "    temp = np.sign(w)\n",
    "    grad = (X.T@X@w - X.T@y)/len(X) + (alpha*temp)\n",
    "    \n",
    "    return grad\n",
    "\n",
    "objective_lasso_derivative(X,y,w,0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F8BaS8g5cYIE"
   },
   "source": [
    "We can check it again, numerically. The two pairs of parameters should be the same:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "id": "V0aMgXBEcYIE"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 6.64666667, 12.3       ]), array([ 6.64666667, 12.3       ]))"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def numerical_objective_lasso_derivative(X,y,w,alpha):\n",
    "    \"\"\"\n",
    "    This finds a numerical approximation to the true gradient\n",
    "    \"\"\"\n",
    "    g = np.zeros_like(w)\n",
    "    for i,wi in enumerate(w):\n",
    "        d = np.zeros_like(w)\n",
    "        d[i]=1e-6\n",
    "        g[i] = (objective_lasso(X,y,w+d,alpha)-objective_lasso(X,y,w-d,alpha))/2e-6\n",
    "    return g\n",
    "\n",
    "objective_lasso_derivative(X,y,w,0.1),numerical_objective_lasso_derivative(X,y,w,0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B-m4LjUHcYIE"
   },
   "source": [
    "### Optimise $\\mathbf{w}$ to minimise the Lasso objective\n",
    "\n",
    "As before we need to optimise to find the optimum value of $\\mathbf{w}$, for this Lasso objective.\n",
    "You'll need to loop lots of times (e.g. 5000). Start with a sensible choice of w. At each iteration: compute the gradient and subtract the scaled gradient from the w parameter (you'll need to scale it by the learning rate, of e.g. 0.05)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "id": "QpsfOgxFcYIF"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.63888889, 0.14259259])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def optimise_parameters_lasso(X,y,startw):\n",
    "    \"\"\"\n",
    "    Returns the w that minimises the Lasso objective.\n",
    "    \"\"\"\n",
    "    #Put answer here\n",
    "    for i in range (5000):\n",
    "        grad =objective_lasso_derivative(X,y,startw,0.1)\n",
    "        startw = startw - grad * 0.05\n",
    "    return startw\n",
    "\n",
    "optimise_parameters_lasso(X,y,w)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GK2UPtJHcYIG"
   },
   "source": [
    "We can check against the sklearn method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "id": "APPYBmk_cYIG"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.63931263 0.1423666 ]\n"
     ]
    }
   ],
   "source": [
    "clf = linear_model.Lasso(alpha=0.1,fit_intercept=False)\n",
    "clf.fit(X,y)\n",
    "print(clf.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "438cqaLtcYIH"
   },
   "source": [
    "The above result should approximately match the one you computed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D5c76jIVcYIH"
   },
   "source": [
    "# Back to air pollution\n",
    "\n",
    "### One-hot-encoding\n",
    "\n",
    "One of the columns isn't numerical, but instead is a string type: The wind direction. The best way to deal with this is one-hot-encoding.\n",
    "\n",
    "pandas has a tool for doing this: `pd.get_dummies(series, prefix='prefix_to_use')`. In our example the series is: `clean_df.wd`.\n",
    "\n",
    "You'll need to:\n",
    "1. Make the one-hot encoding table using the code above.\n",
    "2. Delete the `wd` column from our table (hint: you did this earlier for other columns).\n",
    "3. Join the one hot data to the table. To do this use something like `dataframe1.join(dataframe2)`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "id": "4hUiiPE2cYII"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PM2.5</th>\n",
       "      <th>hour</th>\n",
       "      <th>TEMP</th>\n",
       "      <th>PRES</th>\n",
       "      <th>DEWP</th>\n",
       "      <th>RAIN</th>\n",
       "      <th>WSPM</th>\n",
       "      <th>wd_E</th>\n",
       "      <th>wd_ENE</th>\n",
       "      <th>wd_ESE</th>\n",
       "      <th>...</th>\n",
       "      <th>wd_NNW</th>\n",
       "      <th>wd_NW</th>\n",
       "      <th>wd_S</th>\n",
       "      <th>wd_SE</th>\n",
       "      <th>wd_SSE</th>\n",
       "      <th>wd_SSW</th>\n",
       "      <th>wd_SW</th>\n",
       "      <th>wd_W</th>\n",
       "      <th>wd_WNW</th>\n",
       "      <th>wd_WSW</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>No</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>28434</th>\n",
       "      <td>50.0</td>\n",
       "      <td>17</td>\n",
       "      <td>29.400</td>\n",
       "      <td>998.8</td>\n",
       "      <td>7.9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.4</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34258</th>\n",
       "      <td>272.0</td>\n",
       "      <td>9</td>\n",
       "      <td>-2.575</td>\n",
       "      <td>1016.5</td>\n",
       "      <td>-6.9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.6</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28462</th>\n",
       "      <td>49.0</td>\n",
       "      <td>21</td>\n",
       "      <td>26.100</td>\n",
       "      <td>1000.8</td>\n",
       "      <td>8.6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8235</th>\n",
       "      <td>75.0</td>\n",
       "      <td>2</td>\n",
       "      <td>-1.300</td>\n",
       "      <td>1022.8</td>\n",
       "      <td>-5.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8498</th>\n",
       "      <td>44.0</td>\n",
       "      <td>1</td>\n",
       "      <td>-1.900</td>\n",
       "      <td>1028.4</td>\n",
       "      <td>-9.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.8</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10310</th>\n",
       "      <td>30.0</td>\n",
       "      <td>13</td>\n",
       "      <td>17.500</td>\n",
       "      <td>1010.3</td>\n",
       "      <td>-14.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.2</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9480</th>\n",
       "      <td>54.0</td>\n",
       "      <td>23</td>\n",
       "      <td>19.200</td>\n",
       "      <td>1009.3</td>\n",
       "      <td>-1.3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.6</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28571</th>\n",
       "      <td>83.0</td>\n",
       "      <td>10</td>\n",
       "      <td>25.700</td>\n",
       "      <td>1002.9</td>\n",
       "      <td>9.9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29824</th>\n",
       "      <td>48.0</td>\n",
       "      <td>15</td>\n",
       "      <td>31.900</td>\n",
       "      <td>996.1</td>\n",
       "      <td>25.3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.6</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17940</th>\n",
       "      <td>34.0</td>\n",
       "      <td>11</td>\n",
       "      <td>10.200</td>\n",
       "      <td>1013.3</td>\n",
       "      <td>-11.8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.7</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>29141 rows × 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       PM2.5  hour    TEMP    PRES  DEWP  RAIN  WSPM   wd_E  wd_ENE  wd_ESE  \\\n",
       "No                                                                            \n",
       "28434   50.0    17  29.400   998.8   7.9   0.0   4.4  False   False   False   \n",
       "34258  272.0     9  -2.575  1016.5  -6.9   0.0   1.6  False   False   False   \n",
       "28462   49.0    21  26.100  1000.8   8.6   0.0   3.0  False   False   False   \n",
       "8235    75.0     2  -1.300  1022.8  -5.2   0.0   1.4  False   False   False   \n",
       "8498    44.0     1  -1.900  1028.4  -9.5   0.0   0.8   True   False   False   \n",
       "...      ...   ...     ...     ...   ...   ...   ...    ...     ...     ...   \n",
       "10310   30.0    13  17.500  1010.3 -14.5   0.0   2.2  False   False   False   \n",
       "9480    54.0    23  19.200  1009.3  -1.3   0.0   0.6  False   False   False   \n",
       "28571   83.0    10  25.700  1002.9   9.9   0.0   1.4  False    True   False   \n",
       "29824   48.0    15  31.900   996.1  25.3   0.0   1.6  False   False   False   \n",
       "17940   34.0    11  10.200  1013.3 -11.8   0.0   2.7  False   False   False   \n",
       "\n",
       "       ...  wd_NNW  wd_NW   wd_S  wd_SE  wd_SSE  wd_SSW  wd_SW   wd_W  wd_WNW  \\\n",
       "No     ...                                                                      \n",
       "28434  ...   False  False  False  False   False    True  False  False   False   \n",
       "34258  ...   False  False  False  False   False   False  False  False   False   \n",
       "28462  ...   False  False  False  False   False    True  False  False   False   \n",
       "8235   ...   False  False   True  False   False   False  False  False   False   \n",
       "8498   ...   False  False  False  False   False   False  False  False   False   \n",
       "...    ...     ...    ...    ...    ...     ...     ...    ...    ...     ...   \n",
       "10310  ...   False  False  False  False   False   False  False   True   False   \n",
       "9480   ...   False  False  False  False   False   False  False  False    True   \n",
       "28571  ...   False  False  False  False   False   False  False  False   False   \n",
       "29824  ...    True  False  False  False   False   False  False  False   False   \n",
       "17940  ...   False  False  False  False   False   False  False  False   False   \n",
       "\n",
       "       wd_WSW  \n",
       "No             \n",
       "28434   False  \n",
       "34258   False  \n",
       "28462   False  \n",
       "8235    False  \n",
       "8498    False  \n",
       "...       ...  \n",
       "10310   False  \n",
       "9480    False  \n",
       "28571   False  \n",
       "29824   False  \n",
       "17940    True  \n",
       "\n",
       "[29141 rows x 23 columns]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def add_wd_onehot(df):\n",
    "    \"\"\"Add new one-hot encoding set of columns, removes the old column it's made from. Returns new dataframe.\"\"\"\n",
    "     # Get one hot encoding of columns 'vehicleType'\n",
    "    new_df=pd.get_dummies(df.wd, prefix= 'wd')\n",
    "    # Drop column as it is now encoded\n",
    "    new_clean = df.drop(labels= ['wd'], axis=1)\n",
    "    # Join the encoded df\n",
    "    Final_df= new_clean.join(new_df)  \n",
    "    \n",
    "          \n",
    "    return Final_df\n",
    "\n",
    "#you could use this code to see if it's worked?\n",
    "train_df_wdencoded = add_wd_onehot(train_df)\n",
    "train_df_wdencoded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gR9m46ghcYIJ"
   },
   "source": [
    "### Standardise the data \n",
    "\n",
    "Now we need to standardise the data.\n",
    "\n",
    "You could manipulate just some columns by using, for example: `df.iloc[:,1:]` - this returns a dataframe that consists of all but the first column.\n",
    "\n",
    "Feel free to use either tools from `sklearn.preprocessing` or standardise it by, for example, using the mean and the standard deviation of the columns by calling `some_dataframe.mean()` or `some_dataframe.std()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "id": "ylCMFcVccYIJ"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "def standardise(df):\n",
    "    \"\"\"\n",
    "    Returns a new dataframe in which all but the PM2.5 columns are standardised (i.e. have a mean of zero and standard deviation of 1)\n",
    "\n",
    "    Think about if you want to standardise using the *training* data's mean and standard deviation (for the test data).\n",
    "    \"\"\"\n",
    "   \n",
    "    new_df = df.iloc[:,1:]\n",
    "    temp_df = df.iloc[:,0].to_frame()\n",
    "    \n",
    "    Final_df = (new_df - new_df.mean())/ new_df.std()  \n",
    "    Final_df= temp_df.join(Final_df)  \n",
    "    \n",
    "    return Final_df\n",
    "\n",
    "train_df_preprocessed = standardise(add_wd_onehot(train_df))\n",
    "test_df_preprocessed = standardise(add_wd_onehot(test_df))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7cUc5TFLcYIJ"
   },
   "source": [
    "Here we put the training and test inputs (X) and outputs (y) into four variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "id": "9reEdwwjcYIJ"
   },
   "outputs": [],
   "source": [
    "X = train_df_preprocessed.iloc[:,1:].to_numpy()\n",
    "y = train_df_preprocessed.iloc[:,0].to_numpy()\n",
    "\n",
    "Xtest = test_df_preprocessed.iloc[:,1:].to_numpy()\n",
    "ytest = test_df_preprocessed.iloc[:,0].to_numpy()\n",
    "\n",
    "clf = linear_model.Lasso(alpha=0.1,fit_intercept=True)\n",
    "clf.fit(X,y)\n",
    "\n",
    "Train = clf.predict(X)\n",
    "Test = clf.predict(Xtest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7Cbnx-YWcYIK"
   },
   "source": [
    "We can use the same code we wrote before, using the Lasso from sklearn to fit the data. Here we'll turn fit_intercept on, as we've not added a '1's column to our design matrix.\n",
    "\n",
    "So feel free to use:\n",
    "```\n",
    "clf = linear_model.Lasso(alpha=0.1,fit_intercept=True)\n",
    "clf.fit(X,y)\n",
    "```\n",
    "\n",
    "### Finding the RMSE of the Lasso regressor predictions\n",
    "\n",
    "Next compute the **RMSE** of the predictions for (a) the training data and (b) the test data.\n",
    "The RMSE (root mean squared error) could be computed, for example with:\n",
    "\n",
    "```np.sqrt(np.mean((predicted_values-true_values)**2))```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "id": "m6HCR6qWcYIK"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(74.78698078293763, 74.42378530499064)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "rmse_lasso_train = np.sqrt(np.mean((Train - y )**2))\n",
    "rmse_lasso_test = np.sqrt(np.mean((Test - ytest)**2))\n",
    "\n",
    "rmse_lasso_train, rmse_lasso_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QzqNV3ticYIL"
   },
   "source": [
    "We can compare this to the standard deviation of the data, we should do better than that!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "id": "NYXvIHwMcYIL"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(85.99145204106279, 85.98333117675988)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.std(y), np.std(ytest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qMNAk_HrcYIL"
   },
   "source": [
    "### Random Forest \n",
    "\n",
    "The final step is to use a random forest regressor.\n",
    "\n",
    "If we use the default random forest regressor, we find we get considerable over-fitting. So we need to explore different parameters. We will use a cross-validated grid search over the parameters:\n",
    "\n",
    "- max_features: The number of features to consider when looking for the best split (i.e. controls subsampling), *from 1 to the number of features* in 4 steps (e.g. use `np.linspace`)\n",
    "- n_estimators: The number of trees in the forest, from 10 to 100 in 4 steps.\n",
    "- max_samples : the number of samples to draw from to train each base estimator, from 0.1 to 0.9 in 4 steps.\n",
    "\n",
    "We will use `GridSearchCV`.\n",
    "\n",
    "Have a look at the documentation for this, the three parameters we need to specify are:\n",
    "\n",
    "- the 'estimator': an **INSTANCE** of RandomForestRegressor.\n",
    "- param_grid: a **DICTIONARY**, each item is the title of the parameter, and equals an array of the values we need to test. For exmaple, one of the items might be `{'max_samples': np.linspace(0.1, 0.9, 5)}`.\n",
    "- You'll need to think carefully how to make the lists for the `max_features` and `n_estimators` as these both need to be (positive) integers. E.g. use `.astype(int)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "id": "vObtrvKxcYIM"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>GridSearchCV(estimator=RandomForestRegressor(),\n",
       "             param_grid={&#x27;max_features&#x27;: array([ 1,  8, 15, 22]),\n",
       "                         &#x27;max_samples&#x27;: array([0.1       , 0.36666667, 0.63333333, 0.9       ]),\n",
       "                         &#x27;n_estimators&#x27;: array([ 10,  40,  70, 100])},\n",
       "             scoring=&#x27;neg_mean_squared_error&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" ><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GridSearchCV</label><div class=\"sk-toggleable__content\"><pre>GridSearchCV(estimator=RandomForestRegressor(),\n",
       "             param_grid={&#x27;max_features&#x27;: array([ 1,  8, 15, 22]),\n",
       "                         &#x27;max_samples&#x27;: array([0.1       , 0.36666667, 0.63333333, 0.9       ]),\n",
       "                         &#x27;n_estimators&#x27;: array([ 10,  40,  70, 100])},\n",
       "             scoring=&#x27;neg_mean_squared_error&#x27;)</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-5\" type=\"checkbox\" ><label for=\"sk-estimator-id-5\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: RandomForestRegressor</label><div class=\"sk-toggleable__content\"><pre>RandomForestRegressor()</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-6\" type=\"checkbox\" ><label for=\"sk-estimator-id-6\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomForestRegressor</label><div class=\"sk-toggleable__content\"><pre>RandomForestRegressor()</pre></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "GridSearchCV(estimator=RandomForestRegressor(),\n",
       "             param_grid={'max_features': array([ 1,  8, 15, 22]),\n",
       "                         'max_samples': array([0.1       , 0.36666667, 0.63333333, 0.9       ]),\n",
       "                         'n_estimators': array([ 10,  40,  70, 100])},\n",
       "             scoring='neg_mean_squared_error')"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#1. Create a grid of parameter values for n_estimators, max_features and max_samples,\n",
    "\n",
    "max_features=np.linspace(1, X.shape[1], 4).astype(int)\n",
    "n_estimators=np.linspace(10, 100, 4).astype(int)\n",
    "max_samples=np.linspace(0.1, 0.9, 4)\n",
    "\n",
    "#2. Create a GridSearchCV object, using the random forest regressor:\n",
    "param_grid = dict(n_estimators = n_estimators,  max_samples=max_samples , max_features = max_features)\n",
    "grid_regression = GridSearchCV(RandomForestRegressor(), param_grid=param_grid, scoring='neg_mean_squared_error')\n",
    "\n",
    "\n",
    "#Note: Because there is so much training data, using the full dataset takes too long. So here we'll just use 10%\n",
    "np.random.seed(42)\n",
    "idx = np.sort(np.random.choice(len(X), size=int(len(X)*0.1), replace=False))\n",
    "#3. Fit to training data in (the subset of) X and y\n",
    "grid_regression.fit(X[idx,:],y[idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HUuX3bOYcYIM"
   },
   "source": [
    "Here we print the best parameters from the grid search (on the training/validation cross-validation run):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "id": "ShaXFzjQcYIN"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best n_estimator 100\n",
      "Best max features 15\n",
      "Best max samples 0.6333333333333333\n"
     ]
    }
   ],
   "source": [
    "best_n_estimators = grid_regression.best_params_['n_estimators']\n",
    "best_max_features = grid_regression.best_params_['max_features']\n",
    "best_max_samples = grid_regression.best_params_['max_samples']\n",
    "print('Best n_estimator', best_n_estimators)\n",
    "print('Best max features', best_max_features)\n",
    "print('Best max samples', best_max_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dczjj2ercYIN"
   },
   "source": [
    "### RMSE for the Random Forest Regressor \n",
    "\n",
    "Finally compute the RMSE for the training and test data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "id": "1SAtgw13cYIO"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31.36099223867163\n",
      "59.764234244276764\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(None, None)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Training = RandomForestRegressor(n_estimators=best_n_estimators,max_samples=best_max_samples,max_features = best_max_features)\n",
    "Training.fit(X,y)\n",
    "\n",
    "T = Training.predict(X)\n",
    "y_pred = Training.predict(Xtest)\n",
    "\n",
    "\n",
    "rmse_rf_train = np.sqrt(np.mean((T - y )**2))\n",
    "rmse_rf_test = np.sqrt(np.mean((y_pred- ytest)**2))\n",
    "\n",
    "print(rmse_rf_train), print(rmse_rf_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S_j3g8cycYIP"
   },
   "source": [
    "We can compare this to the standard deviations for the two sets of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "id": "FRjHEn1pcYIQ"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(85.99145204106279, 85.98333117675988)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.std(y), np.std(ytest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wpY-t0TfcYIQ"
   },
   "source": [
    "### Did the random forest do better than lasso regression?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "id": "KOBVQyVpcYIR"
   },
   "outputs": [],
   "source": [
    "#Yes, The RMSE for random forest is much less. However both model did well when RMSE is compared to standard deviation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
